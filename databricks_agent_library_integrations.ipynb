{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c72a2fbe-6916-4e68-9d07-d5ff725c9cd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Building Agents with the Foundation Models API\n",
    "\n",
    "This short notebooks is intended to help you start building agents with models available through the foundation models API (which you can use with your hackathon credits) and popular agent-building frameworks. It will also show how to use your own API keys to access outside models, if you so choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da94478e-ee21-4013-85d5-afcc89cd54da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Getting Started—Notes\n",
    "\n",
    "- We recommend using `databricks-llama-4-maverick` as the base model for building your agents—it is a fast and highly capable model. If you encounter any compatibility issues, you can try `databricks-meta-llama-3-3-70b-instruct` as an alternative, or configure an [external model](#bring-your-own-model).\n",
    "- Though Anthropic's Claude models are listed in the Serving menu, they are not currently usable in trial accounts. If you add your credit card information, you can use the Claude models via Databricks. This will still consume your credits and not charge your card directly (though you will be charged if you consume all of your trial credits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5663bdb9-4982-4424-b375-e734e05fdef8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## LlamaIndex\n",
    "\n",
    "- See the [llamaindex docs](https://docs.llamaindex.ai/en/stable/examples/llm/databricks/) for more details on getting started with LlamaIndex and Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dd069cd-ab52-4cda-a666-279cce387e2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Setup: Install Dependencies\n",
    "\n",
    "We will install the llamaindex package as well as the `llama-index-llms-databricks` package, which lets us configure and use models from Databricks model serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2bc6c79-bc49-4a57-9535-40b85d855756",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U llama-index llama-index-llms-databricks mlflow\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82ec4b99-45e4-4c42-8e78-b66c1d62d832",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure your personal access token\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient()\n",
    "tmp_token = w.tokens.create(comment=\"for model serving\", lifetime_seconds=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fb2baab-dd88-4fbd-b8eb-f05e6fbe69fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Set up your Databricks model with LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23f5874e-105d-40fe-ad36-978d2904f133",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.databricks import Databricks\n",
    "\n",
    "llm = Databricks(\n",
    "    model=\"databricks-llama-4-maverick\",\n",
    "    api_key=tmp_token.token_value,\n",
    "    api_base=f\"{w.config.host}/serving-endpoints/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8a718ac-09b5-4d73-86c8-db968e305f5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Query your Model\n",
    "\n",
    "You can query the llamaindex-configured Databricks model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78c7aa57-c460-4a47-a6c8-09bff5db60d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Completion\n",
    "llm.complete(\"Hello, world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0d3406a-1b57-4dc8-99f3-642797da1009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Chat\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a helpful assistant\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"Hello, world\"),\n",
    "]\n",
    "llm.chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e9362c2-3bdd-4387-ae5d-eefc5aae5637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create a LlamaIndex Agent\n",
    "\n",
    "We can use the llm we configured to create a [Llamaindex Agent](https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/).\n",
    "\n",
    "Note that the Databricks model is compatible with `ReActAgent` but not with `FunctionAgent`. You can still easily use function calling with `ReActAgent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ed98835-ae8e-41f6-8701-65aa8afd5db1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import ReActAgent\n",
    "\n",
    "\n",
    "# Define a simple calculator tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Useful for multiplying two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "# Create an agent workflow with our calculator tool\n",
    "agent = ReActAgent(\n",
    "    tools=[multiply],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that can multiply two numbers. Always explain your answer in the final output. Tell the user if you called on any tools.\",\n",
    ")\n",
    "\n",
    "response = await agent.run(\"What is 1234 * 4567? Tell me how you got the answer.\")\n",
    "\n",
    "response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c8ea980-8380-4ff0-a1d2-2f52ffaa344a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can find more integrations for LlamaIndex, including for [Databricks Vector Search](https://llamahub.ai/l/vector_stores/llama-index-vector-stores-databricks?from=), at [llamahub.ai](https://llamahub.ai/). This is the best source for finding tools to connect your agent to e.g. web search tools, vector stores, additional LLM providers, and more. For example, here's how we can enable DuckDuckGo web search for our agent.\n",
    "\n",
    "We will also trace this interaction with mlflow, making it easy to keep track of all the steps of the agent execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9084fb1-9ef6-451b-9ca3-177c3e742bb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install llama-index-tools-duckduckgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4860ca8-9ab8-4854-9a7f-5e03812d89c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.tools.duckduckgo import DuckDuckGoSearchToolSpec\n",
    "import mlflow\n",
    "\n",
    "mlflow.llama_index.autolog()\n",
    "\n",
    "tool_spec = DuckDuckGoSearchToolSpec()\n",
    "search_tool_list = tool_spec.to_tool_list()\n",
    "\n",
    "agent = ReActAgent(\n",
    "    tools=search_tool_list,\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that can search the web. Answer questions for the user based on web searches.\",\n",
    ")\n",
    "\n",
    "response = await agent.run(\"What is the largest city in Croatia?\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1b51fb1-3031-4115-b8be-f81026741e2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This should be enough to get you started building agents with LlamaIndex and Databricks model serving. As a quick recap:\n",
    "- Use the LlamaIndex `ReActAgent` to get started. The `FunctionAgent` does not currently work with Databricks model serving.\n",
    "- Get additional tools and connections for your agent from llamahub.ai\n",
    "- Use Databricks model serving models with LlamaIndex via the `llama-index-llms-databricks` package\n",
    "- Trace your LlamaIndex interactions with `mlflow.llama_index.autolog()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01431981-a87e-44fb-88a4-418ee1f3a045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## LangGraph\n",
    "\n",
    "You can learn more about the Databricks langchain/langgraph integration [here](https://python.langchain.com/docs/integrations/providers/databricks/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7734aad-61f1-4e12-a018-c343846cc753",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fab99b87-7d6b-46a5-9eb4-8b2ee778b6b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq databricks-langchain langgraph langchain mlflow \n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfb6dc9f-6d6f-4c15-a8e5-51f6f101f361",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Set up your Databricks model with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cce60e2-b4e6-450a-85b4-a80cf66ecb51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks_langchain import ChatDatabricks\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import os\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "os.environ[\"DATABRICKS_HOST\"] = w.config.host\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = w.tokens.create(comment=\"for model serving\", lifetime_seconds=1200).token_value\n",
    "\n",
    "llm = ChatDatabricks(endpoint=\"databricks-llama-4-maverick\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21bf92bd-3b44-489f-b31c-e0905f3a71d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Query your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2ce0807-090a-42b1-a36e-192dc7f25e38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "llm.invoke(\"hello, world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a270f251-dd52-4ab3-a9d0-6a687069bf53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create a LangGraph Agent\n",
    "\n",
    "Note that `databricks-llama-4-maverick` is not currently compatible with multi-turn tool calling, so we will instead use `databricks-meta-llama-3-3-70b-instruct`, which will work appropriately with LangGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c85592ce-61ac-4503-9bd1-b9a9e4bb04b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "llm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-3-70b-instruct\")\n",
    "\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Useful for multiplying two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=llm,  \n",
    "    tools=[multiply],  \n",
    "    prompt=\"You are a helpful assistant\"  \n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is 1267 times 842\"}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6b43617-2de2-442c-8922-16d77532d913",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "As we did with the LlamaIndex agent, we will now:\n",
    "\n",
    "- Add web search to our LangGraph agent\n",
    "- trace our agent execution with mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "283e1ce2-18f7-41ef-9395-18c842f4f925",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "import mlflow\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=llm,  \n",
    "    tools=[search],  \n",
    "    prompt=\"You are a helpful assistant that can search the web. Answer questions for the user based on web searches.\"  \n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the largest city in Croatia?\"}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6675b83a-78a4-4366-adf8-9f74a9560b91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bring your own Model\n",
    "\n",
    "If you want to use models not available through Databricks model serving, or otherwise want to use your own API keys with external model providers, you can do so in a couple of different ways. This might be a good idea if you are running into any compatiblity issues with various agent-building libraries and integrations. OpenAI models in particular tend to be highly compatible with external tools, and models such as `gpt-4o` and `gpt-4o-mini` are relatively inexpensive and still quite capable.\n",
    "\n",
    "1. **Use the model provider's client directly.** This is the same as if you were using the model provider locally. You will need to install the relevant libraries and supply your own API key. e.g.\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = <your_api_key>\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello, World\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "2. **Configure the model as an external model in Databricks model serving**: See [here](https://docs.databricks.com/aws/en/generative-ai/external-models/) for details.\n",
    "    1. Navigate to the `Serving` tab in the left sidebar and click \"Create Serving Endpoint\"\n",
    "    2. Name the endpoint whatever you want in the \"Name\" field\n",
    "    3. Click \"Select an entity\" under \"Entity Details.\" Leave \"Foundation models\" selected.\n",
    "    4. From the \"Select a foundation model\" dropdown, under \"External model providers,\" select your model provider (e.g. OpenAI)\n",
    "    5. Fill out the required details, including the API key and provider model and, optionally, any [AI Gateway configurations](https://docs.databricks.com/aws/en/ai-gateway/) you would like.\n",
    "    6. Query the model in the same way as demonstrated above, using your Databricks workspace token and Databricks model serving integrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9f8be60-9b83-425b-8304-c9ede0e76ffc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### External Models—quick example\n",
    "\n",
    "Suppose you have created a serving endpoint for OpenAI's gpt-4o-mini using the steps detailed above. Let's use this with LangGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0534d4f-e91e-4257-ad29-99326d536d72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "import os\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "os.environ[\"DATABRICKS_HOST\"] = w.config.host\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = w.tokens.create(comment=\"for model serving\", lifetime_seconds=1200).token_value\n",
    "\n",
    "llm = ChatDatabricks(endpoint=\"4omini\") # the name given to the endpoint in step 2 above\n",
    "\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Useful for multiplying two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=llm,  \n",
    "    tools=[multiply],  \n",
    "    prompt=\"You are a helpful assistant\"  \n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is 1267 times 842\"}]}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Agent Frameworks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
